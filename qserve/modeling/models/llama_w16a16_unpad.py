# File authors: Haotian Tang, Shang Yang, Yujun Lin, Song Han
# @article{lin2024qserve,
#   title={QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving},
#   author={Lin*, Yujun and Tang*, Haotian and Yang*, Shang and Zhang, Zhekai and Xiao, Guangxuan and Gan, Chuang and Han, Song},
#   year={2024}
# }

# Inspired by the following papers:
# @article{touvron2023llama,
#   title={Llama 2: Open foundation and fine-tuned chat models},
#   author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
#   journal={arXiv preprint arXiv:2307.09288},
#   year={2023}
# }

# @article{touvron2023llama,
#   title={Llama: Open and efficient foundation language models},
#   author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
#   journal={arXiv preprint arXiv:2302.13971},
#   year={2023}
# }


from typing import Dict, List, Optional

import qserve_backend.fused_attention as fused_attention

# import gc
import torch
from flash_attn.flash_attn_interface import flash_attn_varlen_func
from qserve_backend import fused_kernels
import torch
from torch import nn
from transformers import LlamaConfig

import qserve.utils.constants
from qserve.modeling.layers.activation import SiluAndMul
from qserve.modeling.layers.layernorm import RMSNorm, RMSNormGeneral
from qserve.modeling.layers.quantized_linear import W4A8OF16LinearDynamicInputScale
from qserve.modeling.layers.sampler import Sampler
from qserve.sampling_params import SamplingParams
from qserve.utils.input_metadata import InputMetadata
from qserve.utils.quant_config import QServeQuantConfig
from qserve.utils.weight_utils import (
    convert_pyslice_to_tensor,
    hf_model_weights_iterator,
    load_padded_tensor_parallel_vocab,
    load_tensor_parallel_weights,
)

max_seq_len = qserve.utils.constants.max_seq_len


class LlamaMLP(nn.Module):
    def __init__(self, args) -> None:
        super().__init__()
        hidden_size = args.hidden_size
        intermediate_size = args.intermediate_size
        self.use_int8 = True

        self.gate_up_proj = nn.Linear(
            hidden_size, 2 * intermediate_size, bias=False,
        )
        self.down_proj = nn.Linear(
            intermediate_size, hidden_size, bias=False,
        )

        self.act_fn = SiluAndMul()

    def forward(self, x):
        # INT8 in, FP16 out
        gate_up = self.gate_up_proj(x)
        x = self.act_fn(gate_up)
        x = self.down_proj(x)
        return x 


class LlamaAttention(nn.Module):
    def __init__(
        self,
        args,
        layer_idx: int,
        kv_cache_config: Optional[Dict] = None,
    ) -> None:
        super().__init__()
        hidden_size = args.hidden_size
        num_heads = args.num_attention_heads
        num_kv_heads = args.num_key_value_heads
        rope_theta = getattr(args, "rope_theta", 10000)
        rope_scaling = getattr(args, "rope_scaling", None)
        max_position_embeddings = args.max_position_embeddings

        self.layer_idx = layer_idx

        self.hidden_size = hidden_size
        tp_size = 1
        self.total_num_heads = num_heads
        assert self.total_num_heads % tp_size == 0
        self.num_heads = self.total_num_heads // tp_size
        self.total_num_kv_heads = num_kv_heads
        if self.total_num_kv_heads >= tp_size:
            # Number of KV heads is greater than TP size, so we partition
            # the KV heads across multiple tensor parallel GPUs.
            assert self.total_num_kv_heads % tp_size == 0
        else:
            # Number of KV heads is less than TP size, so we replicate
            # the KV heads across multiple tensor parallel GPUs.
            assert tp_size % self.total_num_kv_heads == 0
        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
        num_kv_heads_replicas = max(1, tp_size // self.total_num_kv_heads)
        self.head_dim = hidden_size // self.total_num_heads
        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim
        self.scaling = self.head_dim**-0.5
        self.rope_theta = rope_theta
        self.max_position_embeddings = max_position_embeddings
        self.use_int8 = True

        if kv_cache_config is None:
            self.kv_cache_config = {"INT4_ENABLED": False, "ZEROS_ENABLED": False}
            print("[Warning] kv cache config is not provided, using default config KV8")
        else:
            self.kv_cache_config = kv_cache_config

        if hasattr(args, "attention_bias"):
            attention_bias = args.attention_bias
        else:
            attention_bias = False
        self.qkv_proj = nn.Linear(
            hidden_size,
            (self.total_num_heads + 2 * self.total_num_kv_heads * num_kv_heads_replicas)
            * self.head_dim,
            bias=attention_bias,
        )

        self.o_proj = nn.Linear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=attention_bias,
        )

        self.kv_max_seq_len = min(max_seq_len, self.max_position_embeddings)

        # self.invoke_quant = self.invoke_quant_wo_act_sum

    # def invoke_quant_wo_act_sum(self, activation_buffer, attn_output):
    #     fused_kernels.invoke_quant(
    #         activation_buffer.quantized_hidden_states_buffer,
    #         attn_output,
    #         activation_buffer.quantized_scale_buffer,
    #     )

    def forward(
        self,
        hidden_states,
        input_metadata: InputMetadata,
    ):
        qkv = self.qkv_proj(hidden_states)
        # print(f"act dtype: {hidden_states.dtype}, weight dtype: {self.qkv_proj.weight.dtype}")
        
        # qkv = qkv.half()
        if input_metadata.is_prompt:
            # Note: the conversion of kv_scale_orig_quant is currently important
            # by default, self.kv_scale_orig_quant will have the same dtype as the model.
            # but the kernel requires float.
            fused_attention.apply_bias_rope_update_kv_cache(
                qkv,
                input_metadata.context_lens,
                input_metadata.padding_offsets,  # size [batch_size, max_seq_len]
                input_metadata.block_tables[self.layer_idx],
                self.num_heads,
                self.num_kv_heads,
                input_metadata.max_seq_len,
                64,  # tokens_per_block
                self.num_kv_heads
                * self.head_dim
                * (1 if self.use_int8 else 2)
                // (2 if self.kv_cache_config["INT4_ENABLED"] else 1),
                self.head_dim,
                self.rope_theta,
                self.max_position_embeddings,
                True,  # neox style
                self.kv_cache_config["INT4_ENABLED"],  # int4_kv
                self.kv_cache_config["ZEROS_ENABLED"],  # kv_cache_with_zeros
            )

            # FIXME: currently qkv share same scale, plan to use seperate scales
            q, k, v = qkv.split(
                [self.q_size, self.kv_size, self.kv_size], dim=-1
            )
            q = q.reshape(q.size(0), self.total_num_heads, self.head_dim)
            k = k.reshape(k.size(0), self.num_kv_heads, self.head_dim)
            v = v.reshape(v.size(0), self.num_kv_heads, self.head_dim)

            attn_output = flash_attn_varlen_func(
                q,
                k,
                v,
                cu_seqlens_q=input_metadata.cu_seqlens,
                cu_seqlens_k=input_metadata.cu_seqlens,
                max_seqlen_q=input_metadata.max_seq_len,
                max_seqlen_k=input_metadata.max_seq_len,
                dropout_p=0.0,
                causal=True,
            )
            attn_output = attn_output.reshape(q.size(0), -1)
        else:
            q, k, v = qkv.split(
                [self.q_size, self.kv_size, self.kv_size], dim=-1
            )
            # k_cache, v_cache = kv_cache
            q = q.reshape(q.size(0), self.total_num_heads, self.head_dim)
            k = k.reshape(k.size(0), self.num_kv_heads, self.head_dim)
            v = v.reshape(v.size(0), self.num_kv_heads, self.head_dim)
            kv_pointers = input_metadata.block_tables[self.layer_idx]
            lengths_per_sample = input_metadata.context_lens  # + 1
            alibi_slopes = None
            memory_max_len = self.kv_max_seq_len
            tokens_per_block = 64
            size_per_token = (
                self.num_kv_heads * self.head_dim * (1 if self.use_int8 else 2)
            )  # size per token for kv cache
            timestep = input_metadata.max_seq_len
            rotary_embedding_dim = self.head_dim
            rotary_base = self.rope_theta
            neox_rotary_style = True
            # shape is (#tokens, #heads, head_dim)
            attn_output = fused_attention.single_query_attention(
                q,
                k,
                v,
                kv_pointers,
                lengths_per_sample,
                alibi_slopes,
                memory_max_len,
                tokens_per_block,
                size_per_token // (2 if self.kv_cache_config["INT4_ENABLED"] else 1),
                timestep,
                rotary_embedding_dim,
                rotary_base,
                neox_rotary_style,
                self.kv_cache_config["INT4_ENABLED"],  # int4_kv
                self.kv_cache_config["ZEROS_ENABLED"],  # kv_cache_with_zeros
            )
            attn_output = attn_output.reshape(q.size(0), -1)
        # # FP16 in, INT8 out
        # self.invoke_quant(activation_buffer, attn_output)
        # # INT8 in, FP16 out
        output = self.o_proj(attn_output)
        return output


class LlamaDecoderLayer(nn.Module):
    def __init__(
        self,
        config: LlamaConfig,
        layer_idx: int,
        kv_cache_config: Optional[Dict] = None,
    ) -> None:
        super().__init__()
        self.hidden_size = config.hidden_size
        self.use_int8 = True    # NOTE: INT8 kv cache
        # Requires transformers > 4.32.0
        rope_theta = getattr(config, "rope_theta", 10000)
        rope_scaling = getattr(config, "rope_scaling", None)
        max_position_embeddings = getattr(config, "max_position_embeddings", 8192)
        self.self_attn = LlamaAttention(
            config,
            layer_idx=layer_idx,
            kv_cache_config=kv_cache_config,
        )
        self.mlp = LlamaMLP(config)

        self.input_layernorm = RMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps,
            use_quant=False,
        )
        self.post_attention_layernorm = RMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps,
            use_quant=False,
        )


    def forward(
        self,
        hidden_states: torch.Tensor,
        input_metadata: InputMetadata,
    ) -> torch.Tensor:
        # FP16 in FP16 out
        # Self Attention
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        hidden_states = self.self_attn(hidden_states, input_metadata)
        hidden_states = residual + hidden_states
        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states


class LlamaModel(nn.Module):
    def __init__(
        self,
        config: LlamaConfig,
        quant_kv_cache: bool = True,
        kv_cache_config: Optional[Dict] = None,
    ) -> None:
        super().__init__()
        self.config = config
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        vocab_size = config.vocab_size
        self.embed_tokens = nn.Embedding(
            vocab_size,
            config.hidden_size,
        )
        self.layers = nn.ModuleList(
            [
                (
                    LlamaDecoderLayer(config, i, kv_cache_config)
                    if quant_kv_cache
                    else None
                )
                for i in range(config.num_hidden_layers)
            ]
        )
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        input_ids: torch.Tensor,
        input_metadata: InputMetadata,
        inputs_embeds: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        with torch.no_grad():
            if inputs_embeds is None:    # For VLM models
                hidden_states = self.embed_tokens(input_ids)
            else:
                hidden_states = inputs_embeds
            for i in range(len(self.layers)):
                layer = self.layers[i]
                hidden_states = layer(hidden_states, input_metadata)
            hidden_states = self.norm(hidden_states)
        return hidden_states


class LlamaForCausalLM(nn.Module):
    def __init__(
        self,
        config: LlamaConfig,
        sampling_params: SamplingParams,
        quant_config: Optional[QServeQuantConfig] = QServeQuantConfig(weight_bits=4),
        kv_cache_config: Optional[Dict] = None,
        quant_path: Optional[str] = None,
    ) -> None:
        quant_kv_cache = True

        super().__init__()
        self.config = config
        self.quant_config = quant_config
        self.model = LlamaModel(
            config, quant_kv_cache, kv_cache_config=kv_cache_config
        )
        vocab_size = config.vocab_size
        # NOTE: The LM head is not quantized.
        self.lm_head = nn.Linear(config.hidden_size, vocab_size, bias=False)
        self._column_parallel_layers = []
        self._row_parallel_layers = ["o_proj", "down_proj"]
        self.sampler = Sampler(sampling_params)

        hidden_size = config.hidden_size
        num_heads = config.num_attention_heads
        num_kv_heads = config.num_key_value_heads

        self.hidden_size = hidden_size
        tp_size = 1
        self.total_num_heads = num_heads
        assert self.total_num_heads % tp_size == 0
        self.num_heads = self.total_num_heads // tp_size
        self.total_num_kv_heads = num_kv_heads
        if self.total_num_kv_heads >= tp_size:
            # Number of KV heads is greater than TP size, so we partition
            # the KV heads across multiple tensor parallel GPUs.
            assert self.total_num_kv_heads % tp_size == 0
        else:
            # Number of KV heads is less than TP size, so we replicate
            # the KV heads across multiple tensor parallel GPUs.
            assert tp_size % self.total_num_kv_heads == 0
        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)

        self.head_dim = hidden_size // self.total_num_heads
        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim

        if quant_path is not None:
            self.load_weights(quant_path)

    def forward(
        self,
        input_ids: torch.Tensor,
        input_metadata: InputMetadata,
        inputs_embeds: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        hidden_states = self.model(input_ids, input_metadata, inputs_embeds)
        if input_metadata.is_prompt:
            output = self.lm_head(
                hidden_states[input_metadata.cu_seqlens[1:] - 1, :]
            )  # only compute last logits
        else:
            output = self.lm_head(hidden_states)
        return output  # .float()

    def sample(
        self,
        input_ids: torch.Tensor,
        logits: torch.Tensor,
        input_metadata: InputMetadata,
    ):
        return self.sampler(input_ids, logits, input_metadata)

    def load_weights(
        self,
        model_name_or_path: str,
        cache_dir: Optional[str] = None,
        load_format: str = "auto",
        revision: Optional[str] = None,
    ):
        if self.quant_config is None:
            col_weight_suffixes = ["weight"]
            row_weight_suffixes = ["weight"]
        else:
            col_weight_suffixes = self.quant_config.get_col_parallel_tensor_names()
            row_weight_suffixes = self.quant_config.get_row_parallel_tensor_names()

        column_parallel_weights: List[str] = []
        for layer in self._column_parallel_layers:
            for suffix in col_weight_suffixes:
                column_parallel_weights.append(f"{layer}.{suffix}")
        row_parallel_weights: List[str] = []
        for layer in self._row_parallel_layers:
            for suffix in row_weight_suffixes:
                row_parallel_weights.append(f"{layer}.{suffix}")

        # TODO fix the tp parallelism
        # tp_size = get_tensor_model_parallel_world_size()
        # tp_rank = get_tensor_model_parallel_rank()
        tp_size = 1
        tp_rank = 0

        q_proj_shard_size = self.config.hidden_size // tp_size
        num_kv_heads_replicas = max(1, tp_size // self.config.num_key_value_heads)
        num_kv_heads_per_gpu = max(1, self.config.num_key_value_heads // tp_size)
        kv_proj_shard_size = (
            self.config.hidden_size
            // self.config.num_attention_heads
            * num_kv_heads_per_gpu
        )
        attention_weight_specs = [
            # (weight_name, shard_size, offset)
            ("q_proj", q_proj_shard_size, 0),
            ("k_proj", kv_proj_shard_size, q_proj_shard_size),
            ("v_proj", kv_proj_shard_size, q_proj_shard_size + kv_proj_shard_size),
        ]
        state_dict = self.state_dict()

        for name, loaded_weight in hf_model_weights_iterator(
            model_name_or_path, cache_dir, load_format, revision
        ):
            if "rotary_emb.inv_freq" in name:
                continue
            # bias is useless for llama
            if "bias" in name:
                pass
                # continue
            # if "norm" in name:
            #     continue

            packed_dim = None
            is_transposed = False
            if self.quant_config is not None:
                packed_dim = self.quant_config.get_packed_dim(name)
                is_transposed = self.quant_config.is_transposed(name)
            if is_transposed:
                loaded_weight = convert_pyslice_to_tensor(loaded_weight)
                loaded_weight = loaded_weight.T

            is_attention_weight = False
            for weight_name, shard_size, offset in attention_weight_specs:
                if weight_name not in name:
                    continue
                # print(weight_name)
                param = state_dict[name.replace(weight_name, "qkv_proj")]
                if is_transposed:
                    param = param.T

                if packed_dim is not None:
                    shard_dim = 0 if not is_transposed else 1
                    if packed_dim == shard_dim:
                        shard_size //= self.quant_config.pack_factor
                        offset //= self.quant_config.pack_factor

                if weight_name in ["k_proj", "v_proj"]:
                    shard_id = tp_rank // num_kv_heads_replicas
                else:
                    shard_id = tp_rank
                loaded_weight = loaded_weight[
                    shard_size * shard_id : shard_size * (shard_id + 1)
                ]
                if "s2_scales" in name or "s2_zeros" in name:
                    param_slice = param.data[:, offset : offset + shard_size]
                else:
                    param_slice = param.data[offset : offset + shard_size]
                assert param_slice.shape == loaded_weight.shape

                param_slice.copy_(loaded_weight)
                is_attention_weight = True
                break
            if is_attention_weight:
                continue

            is_gate_up_weight = False
            for stride_id, weight_name in enumerate(["gate_proj", "up_proj"]):
                if weight_name not in name:
                    continue
                param = state_dict[name.replace(weight_name, "gate_up_proj")]
                if is_transposed:
                    param = param.T

                if "s2_scales" in name or "s2_zeros" in name:
                    shard_size = param.shape[1] // 2
                    param_slice = param.data[
                        :, stride_id * shard_size : (stride_id + 1) * shard_size
                    ]
                else:
                    shard_size = param.shape[0] // 2
                    param_slice = param.data[
                        shard_size * stride_id : shard_size * (stride_id + 1)
                    ]
                loaded_weight = loaded_weight[
                    shard_size * tp_rank : shard_size * (tp_rank + 1)
                ]
                assert param_slice.shape == loaded_weight.shape
                param_slice.copy_(loaded_weight)
                is_gate_up_weight = True
                break
            if is_gate_up_weight:
                continue

            param = state_dict[name]
            if is_transposed:
                param = param.T

            if "embed_tokens" in name or "lm_head" in name:
                load_padded_tensor_parallel_vocab(param, loaded_weight, tp_rank)
                continue

            load_tensor_parallel_weights(
                param,
                loaded_weight,
                name,
                column_parallel_weights,
                row_parallel_weights,
                tp_rank,
            )
